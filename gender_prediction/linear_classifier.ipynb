{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 12:48:36.559331: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-30 12:48:36.559359: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import pysrt\n",
    "import re\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import logging\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove URLs\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - removes any single character tokens\n",
    "    Parameters\n",
    "    ----------\n",
    "        message : The text message to be preprocessed\n",
    "    Returns\n",
    "    -------\n",
    "        text: The preprocessed text\n",
    "    \"\"\" \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub('https?:\\/\\/[a-zA-Z0-9@:%._\\/+~#=?&;-]*', ' ', text)\n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub('\\$[a-zA-Z0-9]*', ' ', text)\n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub('\\@[a-zA-Z0-9]*', ' ', text)\n",
    "    # Replace everything not a letter or apostrophe with a space\n",
    "    text = re.sub('[^a-zA-Z\\']', ' ', text)\n",
    "    # Remove single letter words\n",
    "    text = ' '.join( [w for w in text.split() if len(w)>1] )\n",
    "    \n",
    "    return text\n",
    "        \n",
    "# Process for all messages\n",
    "df = pd.read_csv(r\"big.csv\",encoding='latin1')\n",
    "preprocessed = []\n",
    "for i in range(0,9499):\n",
    "    message = df['Content'].iloc[i]\n",
    "    processed_Text = preprocess(message)\n",
    "    df.loc[i, 'Content'] = processed_Text\n",
    "    preprocessed.append(str(processed_Text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>FileName</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0000e06e07496624211632e8e264126c.txt</td>\n",
       "      <td>paranoia is settin'in sumbuddy's gonna get hur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>000235a2ba2f48231b7d24e1f08d7878.txt</td>\n",
       "      <td>damn these games kicked my ass today huge wate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>000c4b6e2468f7d528876fd1a6dffd4c.txt</td>\n",
       "      <td>it is better to conquer yourself than to win t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>001187432d2a247562082cd0000dec40.txt</td>\n",
       "      <td>its hot over here lol aloha everyone heading o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>001494c3b74f124a2e3435fff17f376b.txt</td>\n",
       "      <td>he's the lord of all the earth the maker of al...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender                              FileName  \\\n",
       "0       1  0000e06e07496624211632e8e264126c.txt   \n",
       "1       0  000235a2ba2f48231b7d24e1f08d7878.txt   \n",
       "2       0  000c4b6e2468f7d528876fd1a6dffd4c.txt   \n",
       "3       0  001187432d2a247562082cd0000dec40.txt   \n",
       "4       1  001494c3b74f124a2e3435fff17f376b.txt   \n",
       "\n",
       "                                             Content  \n",
       "0  paranoia is settin'in sumbuddy's gonna get hur...  \n",
       "1  damn these games kicked my ass today huge wate...  \n",
       "2  it is better to conquer yourself than to win t...  \n",
       "3  its hot over here lol aloha everyone heading o...  \n",
       "4  he's the lord of all the earth the maker of al...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Content'],df['gender'], random_state=42, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7368421052631579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(logreg, open('model_gender_log_gender.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [label]))\n",
    "    return labeled\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Content'],df['age'], random_state=0, test_size=0.1)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9500/9500 [00:00<00:00, 4980113.49it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5188942.31it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 4305801.60it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5320588.60it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5199098.12it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5029776.32it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5260877.74it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 4884870.42it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5243569.94it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5304298.19it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5334121.55it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5384579.46it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5326990.37it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5207932.04it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5256713.46it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 4959040.20it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5285301.50it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 4916817.37it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5365726.91it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5278300.17it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5231865.55it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5305004.39it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5381670.45it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5127511.00it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5209293.76it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5357070.18it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5288107.23it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 4988218.33it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5300064.91it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 5260183.23it/s]\n",
      "100%|██████████| 9500/9500 [00:00<00:00, 4744687.78it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_224157/1973977972.py:15: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  vectors[i] = model.docvecs[prefix]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "    \n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.32842105263157895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, y_train)\n",
    "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dbow)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(logreg, open('model_gender.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_model = pickle.load(open('model.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 2, 0, 2, 1, 1, 3, 1, 0, 3, 1, 0, 0, 1, 1, 2, 3,\n",
       "       1, 0, 1, 1, 2, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 3, 1, 3, 0,\n",
       "       1, 1, 1, 1, 1, 0, 2, 3, 1, 2, 1, 1, 1, 0, 1, 1, 0, 0, 1, 3, 1, 1,\n",
       "       1, 0, 1, 1, 0, 2, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1,\n",
       "       0, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 0, 1, 1, 1, 2, 2, 1, 1,\n",
       "       3, 1, 0, 3, 1, 2, 0, 1, 0, 1, 1, 1, 2, 1, 1, 0, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 2, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 2, 0, 1, 3, 1, 0, 1, 1, 1, 3, 1, 1, 1, 0, 2, 2, 0, 1, 1,\n",
       "       1, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 0, 0, 2, 3, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 2, 1, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 2, 1, 2, 1, 1, 1, 1, 3, 0, 1, 2, 1, 1,\n",
       "       1, 1, 2, 3, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 3, 0, 0, 2, 1, 1, 1, 1,\n",
       "       2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 0, 2, 2,\n",
       "       1, 0, 3, 1, 0, 2, 0, 3, 2, 3, 1, 0, 1, 1, 1, 2, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 1, 2, 1, 1, 1, 0, 1, 1, 1, 2, 1, 2, 0, 1, 1, 3, 2, 0, 1,\n",
       "       3, 1, 1, 1, 2, 1, 1, 2, 0, 1, 1, 2, 3, 1, 1, 0, 2, 1, 1, 2, 0, 2,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 3, 1, 0, 1, 1, 2, 1, 1,\n",
       "       1, 2, 3, 1, 0, 1, 1, 0, 1, 3, 1, 1, 1, 0, 1, 1, 2, 1, 1, 1, 1, 1,\n",
       "       2, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 0, 2, 1, 1, 1, 3, 0, 1,\n",
       "       1, 1, 1, 2, 3, 2, 1, 1, 1, 0, 3, 2, 2, 0, 1, 2, 0, 1, 1, 1, 1, 1,\n",
       "       2, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 2, 2, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 2, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 3, 3, 1,\n",
       "       1, 3, 1, 1, 1, 1, 1, 0, 0, 3, 1, 0, 1, 1, 1, 1, 1, 3, 2, 0, 0, 3,\n",
       "       3, 1, 1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 3, 2, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 3, 0, 0, 1, 0, 0, 1, 3, 1, 0, 0, 1, 1, 1, 1, 1, 1, 2,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 3, 1, 2, 1, 1, 1, 2, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 2, 2, 3, 1, 2, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
       "       3, 1, 1, 1, 2, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 3, 1, 0, 1, 2, 1, 3, 1, 0, 1, 1, 0, 1, 2, 1, 0,\n",
       "       3, 1, 1, 2, 1, 0, 1, 1, 1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "       3, 0, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 1, 1, 3, 1, 1, 0, 1, 1, 1,\n",
       "       2, 1, 1, 1, 2, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 0, 0, 1,\n",
       "       1, 3, 1, 1, 1, 1, 3, 1, 2, 1, 1, 0, 1, 2, 1, 1, 1, 0, 1, 1, 2, 1,\n",
       "       1, 1, 0, 1, 2, 0, 1, 2, 0, 1, 1, 0, 3, 3, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 0, 1, 1, 2, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 2, 1, 1, 1,\n",
       "       1, 1, 1, 1, 3, 2, 1, 0, 1, 1, 1, 3, 2, 3, 0, 3, 2, 1, 1, 1, 2, 1,\n",
       "       2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 2, 1, 1, 1, 3, 1, 0, 1, 0, 1, 3,\n",
       "       0, 1, 0, 3, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 3, 1, 0, 1, 0, 3, 1, 3, 0,\n",
       "       1, 1, 1, 3, 2, 1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 0, 2, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 3, 1, 1, 1, 1, 0, 0, 1, 2, 3, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 2, 2, 0, 1, 0, 0, 1, 2, 0, 1, 1, 0, 2, 2, 1, 1, 1,\n",
       "       3, 1, 1, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickled_model.predict(test_vectors_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_dbow, open('doc2vec.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_d2v = pickle.load(open('doc2vec.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_224157/1156298670.py:15: DeprecationWarning: Call to deprecated `docvecs` (The `docvecs` property has been renamed `dv`.).\n",
      "  vectors[i] = model.docvecs[prefix]\n"
     ]
    }
   ],
   "source": [
    "test_vectors_dbow = get_vectors(pickled_d2v, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickled_model.predict(test_vectors_dbow[11].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
