{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import pysrt\n",
    "import re\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import logging\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove URLs\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - removes any single character tokens\n",
    "    Parameters\n",
    "    ----------\n",
    "        message : The text message to be preprocessed\n",
    "    Returns\n",
    "    -------\n",
    "        text: The preprocessed text\n",
    "    \"\"\" \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub('https?:\\/\\/[a-zA-Z0-9@:%._\\/+~#=?&;-]*', ' ', text)\n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub('\\$[a-zA-Z0-9]*', ' ', text)\n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub('\\@[a-zA-Z0-9]*', ' ', text)\n",
    "    # Replace everything not a letter or apostrophe with a space\n",
    "    text = re.sub('[^a-zA-Z\\']', ' ', text)\n",
    "    # Remove single letter words\n",
    "    text = ' '.join( [w for w in text.split() if len(w)>1] )\n",
    "    \n",
    "    return text\n",
    "        \n",
    "# Process for all messages\n",
    "df = pd.read_csv(r\"age.csv\",encoding='latin1')\n",
    "preprocessed = []\n",
    "for i in range(0,9499):\n",
    "    message = df['Content'].iloc[i]\n",
    "    processed_Text = preprocess(message)\n",
    "    df.loc[i, 'Content'] = processed_Text\n",
    "    preprocessed.append(str(processed_Text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>FileName</th>\n",
       "      <th>Content</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0000e06e07496624211632e8e264126c.txt</td>\n",
       "      <td>paranoia is settin'in sumbuddy's gonna get hur...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000235a2ba2f48231b7d24e1f08d7878.txt</td>\n",
       "      <td>damn these games kicked my ass today huge wate...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>000c4b6e2468f7d528876fd1a6dffd4c.txt</td>\n",
       "      <td>it is better to conquer yourself than to win t...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>001187432d2a247562082cd0000dec40.txt</td>\n",
       "      <td>its hot over here lol aloha everyone heading o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>001494c3b74f124a2e3435fff17f376b.txt</td>\n",
       "      <td>he's the lord of all the earth the maker of al...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age                              FileName  \\\n",
       "0    1  0000e06e07496624211632e8e264126c.txt   \n",
       "1    1  000235a2ba2f48231b7d24e1f08d7878.txt   \n",
       "2    1  000c4b6e2468f7d528876fd1a6dffd4c.txt   \n",
       "3    1  001187432d2a247562082cd0000dec40.txt   \n",
       "4    1  001494c3b74f124a2e3435fff17f376b.txt   \n",
       "\n",
       "                                             Content Unnamed: 3  \n",
       "0  paranoia is settin'in sumbuddy's gonna get hur...        NaN  \n",
       "1  damn these games kicked my ass today huge wate...        NaN  \n",
       "2  it is better to conquer yourself than to win t...        NaN  \n",
       "3  its hot over here lol aloha everyone heading o...        NaN  \n",
       "4  he's the lord of all the earth the maker of al...        NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5669\n",
       "1    2401\n",
       "2    1045\n",
       "3     385\n",
       "Name: age, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = tf.keras.utils.to_categorical(df[\"age\"].values, num_classes=4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Content'], y, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/itadmin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/itadmin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/itadmin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, option):\n",
    "  '''\n",
    "  Tokenize the input text as per specified option\n",
    "    1: Use python split() function\n",
    "    2: Use regex to extract alphabets plus 's and 't\n",
    "    3: Use NLTK word_tokenize()\n",
    "    4: Use NLTK word_tokenize(), remove stop words and apply lemmatization\n",
    "  '''\n",
    "  if option == 1:\n",
    "    return text.split()\n",
    "  elif option == 2:\n",
    "    return re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text)\n",
    "  elif option == 3:\n",
    "    return [word for word in word_tokenize(text) if (word.isalpha()==1)]\n",
    "  elif option == 4:\n",
    "    words = [word for word in word_tokenize(text) if (word.isalpha()==1)]\n",
    "    # Remove stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "    words = [word for word in words if (word not in stop)]\n",
    "    # Lemmatize words (first noun, then verb)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized = [wnl.lemmatize(wnl.lemmatize(word, 'n'), 'v') for word in words]\n",
    "    return lemmatized\n",
    "  else:\n",
    "    logging.warn(\"Please specify option value between 1 and 4\")\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Define LSTM Model\n",
    "class LstmTextClassifier(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_size, lstm_size, dense_size, output_size, lstm_layers=2, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Initialize the model\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.vocab_size = vocab_size\n",
    "    self.embed_size = embed_size\n",
    "    self.lstm_size = lstm_size\n",
    "    self.dense_size = dense_size\n",
    "    self.output_size = output_size\n",
    "    self.lstm_layers = lstm_layers\n",
    "    self.dropout = dropout\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "    self.lstm = nn.LSTM(embed_size, lstm_size, lstm_layers, dropout=dropout, batch_first=False)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    # Insert an additional fully connected when combining with other inputs\n",
    "    if dense_size == 0:\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "    else:\n",
    "        self.fc1 = nn.Linear(lstm_size, dense_size)\n",
    "        self.fc2 = nn.Linear(dense_size, output_size)\n",
    "\n",
    "    self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    \"\"\"\n",
    "    Initialize the hidden state\n",
    "    \"\"\"\n",
    "    weight = next(self.parameters()).data\n",
    "    hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "              weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "\n",
    "    return hidden\n",
    "\n",
    "  def forward(self, nn_input_text, hidden_state):\n",
    "    \"\"\"\n",
    "    Perform a forward pass of the model on nn_input\n",
    "    \"\"\"\n",
    "    batch_size = nn_input_text.size(0)\n",
    "    nn_input_text = nn_input_text.long()\n",
    "    embeds = self.embedding(nn_input_text)\n",
    "    lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
    "    # Stack up LSTM outputs, apply dropout\n",
    "    lstm_out = lstm_out[-1,:,:]\n",
    "    lstm_out = self.dropout(lstm_out)\n",
    "    # Insert an additional fully connected when combining with other inputs\n",
    "    if self.dense_size == 0:\n",
    "        out = self.fc(lstm_out)\n",
    "    else:\n",
    "        dense_out = self.fc1(lstm_out)\n",
    "        out = self.fc2(dense_out)\n",
    "    # Softmax\n",
    "    logps = self.softmax(out)\n",
    "\n",
    "    return logps, hidden_state\n",
    "      \n",
    "     \n",
    "# Define LSTM Tokenizer\n",
    "def tokenizer_lstm(X, vocab, seq_len, padding):\n",
    "  '''\n",
    "  Returns tokenized tensor with left/right padding at the specified sequence length\n",
    "  '''\n",
    "  X_tmp = np.zeros((len(X), seq_len), dtype=np.int64)\n",
    "  for i, text in enumerate(X):\n",
    "    tokens = tokenize_text(text, 3) \n",
    "    token_ids = [vocab[word] for word in tokens]\n",
    "    end_idx = min(len(token_ids), seq_len)\n",
    "    if padding == 'right':\n",
    "      X_tmp[i,:end_idx] = token_ids[:end_idx]\n",
    "    elif padding == 'left':\n",
    "      start_idx = max(seq_len - len(token_ids), 0)\n",
    "      X_tmp[i,start_idx:] = token_ids[:end_idx]\n",
    "\n",
    "  return torch.tensor(X_tmp, dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Define a DataSet Class which simply return (x, y) pair\n",
    "class SimpleDataset(Dataset):\n",
    "  def __init__(self, x, y):\n",
    "    self.datalist=[(x[i], y[i]) for i in range(len(y))]\n",
    "  def __len__(self):\n",
    "    return len(self.datalist)\n",
    "  def __getitem__(self,idx):\n",
    "    return self.datalist[idx]\n",
    "      \n",
    "# Data Loader\n",
    "def create_data_loader(X, y, indices, batch_size, shuffle):\n",
    "  X_sampled = np.array(X, dtype=object)[indices]\n",
    "  y_sampled = np.array(y)[indices].astype(int)\n",
    "  dataset = SimpleDataset(X_sampled, y_sampled)\n",
    "  loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "  return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def train_cycles(X_all, y_all, vocab, num_samples, model_type, epochs, patience, batch_size, seq_len, lr, clip, log_level):\n",
    "  result = pd.DataFrame(columns=['Accuracy', 'F1(macro)', 'Total_Time', 'ms/text'], index=num_samples)\n",
    "\n",
    "  for n in num_samples:\n",
    "    print(\"\")\n",
    "    logging.info(\"############### Start training for %d samples ###############\" %n)\n",
    "\n",
    "    # Stratified sampling\n",
    "    train_size = n / len(y_all)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=train_size, test_size=train_size*0.2)\n",
    "    train_indices, valid_indices = next(sss.split(X_all, y_all))\n",
    "\n",
    "    # Sample input data\n",
    "    train_loader = create_data_loader(X_all, y_all, train_indices, batch_size, True)\n",
    "    valid_loader = create_data_loader(X_all, y_all, valid_indices, batch_size, False)\n",
    "\n",
    "    if model_type == 'LSTM':\n",
    "      model = LstmTextClassifier(len(vocab)+1, embed_size=512, lstm_size=1024, dense_size=0, output_size = 4, lstm_layers=4, dropout=0.2)\n",
    "      model.embedding.weight.data.uniform_(-1, 1)\n",
    "\n",
    "    start_time = time.perf_counter() # use time.process_time() for CPU time\n",
    "    acc, f1, model_trained = train_nn_model(model, model_type, train_loader, valid_loader, vocab, epochs, patience, batch_size, seq_len, lr, clip, log_level)\n",
    "    end_time = time.perf_counter() # use time.process_time() for CPU time\n",
    "    duration = end_time - start_time\n",
    "    logging.info(\"Process Time (sec): {}\".format(duration))\n",
    "    result.loc[n] = (round(acc,4), round(f1,4), duration, duration/n*1000)\n",
    "\n",
    "  return result, model_trained\n",
    "\n",
    "# Define metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def metric(y_true, y_pred):\n",
    "  acc = accuracy_score(y_true, y_pred)\n",
    "  f1 = f1_score(y_true, y_pred, average='macro')\n",
    "  return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-28 22:30:06.995855: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-01-28 22:30:06.995885: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-01-28 22:30:06.995909: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (455team3): /proc/driver/nvidia/version does not exist\n",
      "2023-01-28 22:30:06.996475: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-28 22:30:16.141311: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(sentences):\n",
    "  '''return BERT-like embeddings of input text\n",
    "  Args:\n",
    "    - sentences: list of strings\n",
    "  Output:\n",
    "    - BERT-like embeddings: tf.Tensor of shape=(len(sentences), 768)\n",
    "  '''\n",
    "  preprocessed_text = bert_preprocess(sentences)\n",
    "  return bert_encoder(preprocessed_text)['pooled_output']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def balanced_recall(y_true, y_pred):\n",
    "    \"\"\"This function calculates the balanced recall metric\n",
    "    recall = TP / (TP + FN)\n",
    "    \"\"\"\n",
    "    recall_by_class = 0\n",
    "    # iterate over each predicted class to get class-specific metric\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        y_pred_class = y_pred[:, i]\n",
    "        y_true_class = y_true[:, i]\n",
    "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true_class, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        recall_by_class = recall_by_class + recall\n",
    "    return recall_by_class / y_pred.shape[1]\n",
    "\n",
    "def balanced_precision(y_true, y_pred):\n",
    "    \"\"\"This function calculates the balanced precision metric\n",
    "    precision = TP / (TP + FP)\n",
    "    \"\"\"\n",
    "    precision_by_class = 0\n",
    "    # iterate over each predicted class to get class-specific metric\n",
    "    for i in range(y_pred.shape[1]):\n",
    "        y_pred_class = y_pred[:, i]\n",
    "        y_true_class = y_true[:, i]\n",
    "        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred_class, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        precision_by_class = precision_by_class + precision\n",
    "    # return average balanced metric for each class\n",
    "    return precision_by_class / y_pred.shape[1]\n",
    "\n",
    "def balanced_f1_score(y_true, y_pred):\n",
    "    \"\"\"This function calculates the F1 score metric\"\"\"\n",
    "    precision = balanced_precision(y_true, y_pred)\n",
    "    recall = balanced_recall(y_true, y_pred)\n",
    "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "i = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "x = bert_preprocess(i)\n",
    "x = bert_encoder(x)\n",
    "x = tf.keras.layers.Dropout(0.1, name=\"dropout\")(x['pooled_output'])\n",
    "x = tf.keras.layers.Dense(num_classes, activation='sigmoid', name=\"output\")(x)\n",
    "\n",
    "model = tf.keras.Model(i, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_text as text\n",
    "\n",
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "metrics = tf.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "init_lr = 3e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfhub_handle_preprocess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m classifier_model \u001b[39m=\u001b[39m build_classifier_model()\n",
      "Cell \u001b[0;32mIn[25], line 10\u001b[0m, in \u001b[0;36mbuild_classifier_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbuild_classifier_model\u001b[39m():\n\u001b[1;32m      9\u001b[0m   text_input \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mstring, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m   preprocessing_layer \u001b[39m=\u001b[39m hub\u001b[39m.\u001b[39mKerasLayer(tfhub_handle_preprocess, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpreprocessing\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m   encoder_inputs \u001b[39m=\u001b[39m preprocessing_layer(text_input)\n\u001b[1;32m     12\u001b[0m   encoder \u001b[39m=\u001b[39m hub\u001b[39m.\u001b[39mKerasLayer(tfhub_handle_encoder, trainable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mBERT_encoder\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfhub_handle_preprocess' is not defined"
     ]
    }
   ],
   "source": [
    "classifier_model = build_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "268/268 [==============================] - 946s 4s/step - loss: 1.0339 - accuracy: 0.5835 - balanced_recall: 0.5141 - balanced_precision: 0.2555 - balanced_f1_score: 0.3387 - val_loss: 0.9810 - val_accuracy: 0.5937 - val_balanced_recall: 0.5012 - val_balanced_precision: 0.2477 - val_balanced_f1_score: 0.3277\n",
      "Epoch 2/40\n",
      "268/268 [==============================] - 953s 4s/step - loss: 1.0095 - accuracy: 0.5870 - balanced_recall: 0.5257 - balanced_precision: 0.2690 - balanced_f1_score: 0.3526 - val_loss: 0.9674 - val_accuracy: 0.6011 - val_balanced_recall: 0.5860 - val_balanced_precision: 0.2870 - val_balanced_f1_score: 0.3830\n",
      "Epoch 3/40\n",
      "268/268 [==============================] - 960s 4s/step - loss: 0.9949 - accuracy: 0.5915 - balanced_recall: 0.5379 - balanced_precision: 0.2793 - balanced_f1_score: 0.3645 - val_loss: 0.9618 - val_accuracy: 0.5905 - val_balanced_recall: 0.5889 - val_balanced_precision: 0.2810 - val_balanced_f1_score: 0.3789\n",
      "Epoch 4/40\n",
      " 39/268 [===>..........................] - ETA: 12:56 - loss: 0.9732 - accuracy: 0.5913 - balanced_recall: 0.5685 - balanced_precision: 0.2756 - balanced_f1_score: 0.3699"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = 8\n",
    "\n",
    "METRICS = [\n",
    "      tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "      balanced_recall,\n",
    "      balanced_precision,\n",
    "      balanced_f1_score\n",
    "]\n",
    "\n",
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", \n",
    "                                                      patience = 5,\n",
    "                                                      restore_best_weights = True)\n",
    "\n",
    "model.compile(optimizer = \"adam\",\n",
    "              loss = \"categorical_crossentropy\",\n",
    "              metrics = METRICS)\n",
    "\n",
    "model_fit = model.fit(X_train, \n",
    "                      y_train, \n",
    "                      epochs = n_epochs,\n",
    "                      validation_data = (X_test, y_test),\n",
    "                      callbacks = [earlystop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (38,) and (5,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(nrows\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, ncols\u001b[39m=\u001b[39mnum_metrics, figsize\u001b[39m=\u001b[39m(\u001b[39m30\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, num_metrics):\n\u001b[0;32m---> 10\u001b[0m   ax[i]\u001b[39m.\u001b[39;49mplot(x, model_fit\u001b[39m.\u001b[39;49mhistory[metric_list[i]], marker\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mo\u001b[39;49m\u001b[39m\"\u001b[39;49m, label\u001b[39m=\u001b[39;49mmetric_list[i]\u001b[39m.\u001b[39;49mreplace(\u001b[39m\"\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m     11\u001b[0m   ax[i]\u001b[39m.\u001b[39mplot(x, model_fit\u001b[39m.\u001b[39mhistory[metric_list[i\u001b[39m+\u001b[39mnum_metrics]], marker\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mo\u001b[39m\u001b[39m\"\u001b[39m, label\u001b[39m=\u001b[39mmetric_list[i\u001b[39m+\u001b[39mnum_metrics]\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     12\u001b[0m   ax[i]\u001b[39m.\u001b[39mset_xlabel(\u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m,fontsize\u001b[39m=\u001b[39m\u001b[39m14\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py:1668\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1425\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1667\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1668\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1669\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1670\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (38,) and (5,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = list(range(1, n_epochs-1))\n",
    "metric_list = list(model_fit.history.keys())\n",
    "num_metrics = int(len(metric_list)/2)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=num_metrics, figsize=(30, 5))\n",
    "\n",
    "for i in range(0, num_metrics):\n",
    "  ax[i].plot(x, model_fit.history[metric_list[i]], marker=\"o\", label=metric_list[i].replace(\"_\", \" \"))\n",
    "  ax[i].plot(x, model_fit.history[metric_list[i+num_metrics]], marker=\"o\", label=metric_list[i+num_metrics].replace(\"_\", \" \"))\n",
    "  ax[i].set_xlabel(\"epochs\",fontsize=14)\n",
    "  ax[i].set_title(metric_list[i].replace(\"_\", \" \"),fontsize=20)\n",
    "  ax[i].legend(loc=\"lower left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_class(reviews):\n",
    "  '''predict class of input text\n",
    "  Args:\n",
    "    - reviews (list of strings)\n",
    "  Output:\n",
    "    - class (list of int)\n",
    "  '''\n",
    "  return [np.argmax(pred) for pred in model.predict(reviews)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-28 09:46:35.873914: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 366). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ~/text_classifier_v1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ~/text_classifier_v1/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"~/text_classifier_v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 366). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://0ff71077-95d8-4dd0-bff7-e3b263802faf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://0ff71077-95d8-4dd0-bff7-e3b263802faf/assets\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'finalized_model_age.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to restore custom object of type _tf_keras_metric. Please make sure that any custom layers are included in the `custom_objects` arg when calling `load_model()` and make sure that all layers implement `get_config` and `from_config`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loaded_model \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(\u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      2\u001b[0m result \u001b[39m=\u001b[39m loaded_model\u001b[39m.\u001b[39mscore(X_test, y_test)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/saving/pickle_utils.py:48\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[0;34m(serialized_model)\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mGFile(dest_path, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     47\u001b[0m         f\u001b[39m.\u001b[39mwrite(archive\u001b[39m.\u001b[39mextractfile(name)\u001b[39m.\u001b[39mread())\n\u001b[0;32m---> 48\u001b[0m model \u001b[39m=\u001b[39m save_module\u001b[39m.\u001b[39;49mload_model(temp_dir)\n\u001b[1;32m     49\u001b[0m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mrmtree(temp_dir)\n\u001b[1;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/saving/saved_model/load.py:1000\u001b[0m, in \u001b[0;36mrevive_custom_object\u001b[0;34m(identifier, metadata)\u001b[0m\n\u001b[1;32m    998\u001b[0m   \u001b[39mreturn\u001b[39;00m revived_cls\u001b[39m.\u001b[39m_init_from_metadata(metadata)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1000\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1001\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnable to restore custom object of type \u001b[39m\u001b[39m{\u001b[39;00midentifier\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1002\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPlease make sure that any custom layers are included in the \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1003\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m`custom_objects` arg when calling `load_model()` and make sure that \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1004\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mall layers implement `get_config` and `from_config`.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to restore custom object of type _tf_keras_metric. Please make sure that any custom layers are included in the `custom_objects` arg when calling `load_model()` and make sure that all layers implement `get_config` and `from_config`."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to restore custom object of type _tf_keras_metric. Please make sure that any custom layers are included in the `custom_objects` arg when calling `load_model()` and make sure that all layers implement `get_config` and `from_config`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mfinalized_model_age.sav\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# dump information to that file\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m data \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(file)\n\u001b[1;32m     10\u001b[0m \u001b[39m# close the file\u001b[39;00m\n\u001b[1;32m     11\u001b[0m file\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/saving/pickle_utils.py:48\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[0;34m(serialized_model)\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mGFile(dest_path, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     47\u001b[0m         f\u001b[39m.\u001b[39mwrite(archive\u001b[39m.\u001b[39mextractfile(name)\u001b[39m.\u001b[39mread())\n\u001b[0;32m---> 48\u001b[0m model \u001b[39m=\u001b[39m save_module\u001b[39m.\u001b[39;49mload_model(temp_dir)\n\u001b[1;32m     49\u001b[0m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mrmtree(temp_dir)\n\u001b[1;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/saving/saved_model/load.py:1000\u001b[0m, in \u001b[0;36mrevive_custom_object\u001b[0;34m(identifier, metadata)\u001b[0m\n\u001b[1;32m    998\u001b[0m   \u001b[39mreturn\u001b[39;00m revived_cls\u001b[39m.\u001b[39m_init_from_metadata(metadata)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1000\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1001\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnable to restore custom object of type \u001b[39m\u001b[39m{\u001b[39;00midentifier\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1002\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mPlease make sure that any custom layers are included in the \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1003\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m`custom_objects` arg when calling `load_model()` and make sure that \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1004\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mall layers implement `get_config` and `from_config`.\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to restore custom object of type _tf_keras_metric. Please make sure that any custom layers are included in the `custom_objects` arg when calling `load_model()` and make sure that all layers implement `get_config` and `from_config`."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "# open a file, where you stored the pickled data\n",
    "file = open(\"finalized_model_age.sav\", 'rb')\n",
    "\n",
    "# dump information to that file\n",
    "data = pickle.load(file)\n",
    "\n",
    "# close the file\n",
    "file.close()\n",
    "\n",
    "print('Showing the pickled data:')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
