{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import logging\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(message):\n",
    "    \"\"\"\n",
    "    This function takes a string as input, then performs these operations: \n",
    "        - lowercase\n",
    "        - remove URLs\n",
    "        - remove ticker symbols \n",
    "        - removes punctuation\n",
    "        - removes any single character tokens\n",
    "    Parameters\n",
    "    ----------\n",
    "        message : The text message to be preprocessed\n",
    "    Returns\n",
    "    -------\n",
    "        text: The preprocessed text\n",
    "    \"\"\" \n",
    "    # Lowercase the twit message\n",
    "    text = message.lower()\n",
    "    # Replace URLs with a space in the message\n",
    "    text = re.sub('https?:\\/\\/[a-zA-Z0-9@:%._\\/+~#=?&;-]*', ' ', text)\n",
    "    # Replace ticker symbols with a space. The ticker symbols are any stock symbol that starts with $.\n",
    "    text = re.sub('\\$[a-zA-Z0-9]*', ' ', text)\n",
    "    # Replace StockTwits usernames with a space. The usernames are any word that starts with @.\n",
    "    text = re.sub('\\@[a-zA-Z0-9]*', ' ', text)\n",
    "    # Replace everything not a letter or apostrophe with a space\n",
    "    text = re.sub('[^a-zA-Z\\']', ' ', text)\n",
    "    # Remove single letter words\n",
    "    text = ' '.join( [w for w in text.split() if len(w)>1] )\n",
    "    \n",
    "    return text\n",
    "        \n",
    "# Process for all messages\n",
    "df = pd.read_csv(r\"age.csv\",encoding='latin1')\n",
    "preprocessed = []\n",
    "for i in range(0,9499):\n",
    "    message = df['Content'].iloc[i]\n",
    "    processed_Text = preprocess(message)\n",
    "    df.loc[i, 'Content'] = processed_Text\n",
    "    preprocessed.append(str(processed_Text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/itadmin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/itadmin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/itadmin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, option):\n",
    "  '''\n",
    "  Tokenize the input text as per specified option\n",
    "    1: Use python split() function\n",
    "    2: Use regex to extract alphabets plus 's and 't\n",
    "    3: Use NLTK word_tokenize()\n",
    "    4: Use NLTK word_tokenize(), remove stop words and apply lemmatization\n",
    "  '''\n",
    "  if option == 1:\n",
    "    return text.split()\n",
    "  elif option == 2:\n",
    "    return re.findall(r'\\b([a-zA-Z]+n\\'t|[a-zA-Z]+\\'s|[a-zA-Z]+)\\b', text)\n",
    "  elif option == 3:\n",
    "    return [word for word in word_tokenize(text) if (word.isalpha()==1)]\n",
    "  elif option == 4:\n",
    "    words = [word for word in word_tokenize(text) if (word.isalpha()==1)]\n",
    "    # Remove stop words\n",
    "    stop = set(stopwords.words('english'))\n",
    "    words = [word for word in words if (word not in stop)]\n",
    "    # Lemmatize words (first noun, then verb)\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized = [wnl.lemmatize(wnl.lemmatize(word, 'n'), 'v') for word in words]\n",
    "    return lemmatized\n",
    "  else:\n",
    "    logging.warn(\"Please specify option value between 1 and 4\")\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Define LSTM Model\n",
    "class LstmTextClassifier(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_size, lstm_size, dense_size, output_size, lstm_layers=2, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Initialize the model\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.vocab_size = vocab_size\n",
    "    self.embed_size = embed_size\n",
    "    self.lstm_size = lstm_size\n",
    "    self.dense_size = dense_size\n",
    "    self.output_size = output_size\n",
    "    self.lstm_layers = lstm_layers\n",
    "    self.dropout = dropout\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "    self.lstm = nn.LSTM(embed_size, lstm_size, lstm_layers, dropout=dropout, batch_first=False)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    # Insert an additional fully connected when combining with other inputs\n",
    "    if dense_size == 0:\n",
    "        self.fc = nn.Linear(lstm_size, output_size)\n",
    "    else:\n",
    "        self.fc1 = nn.Linear(lstm_size, dense_size)\n",
    "        self.fc2 = nn.Linear(dense_size, output_size)\n",
    "\n",
    "    self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def init_hidden(self, batch_size):\n",
    "    \"\"\"\n",
    "    Initialize the hidden state\n",
    "    \"\"\"\n",
    "    weight = next(self.parameters()).data\n",
    "    hidden = (weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_(),\n",
    "              weight.new(self.lstm_layers, batch_size, self.lstm_size).zero_())\n",
    "\n",
    "    return hidden\n",
    "\n",
    "  def forward(self, nn_input_text, hidden_state):\n",
    "    \"\"\"\n",
    "    Perform a forward pass of the model on nn_input\n",
    "    \"\"\"\n",
    "    batch_size = nn_input_text.size(0)\n",
    "    nn_input_text = nn_input_text.long()\n",
    "    embeds = self.embedding(nn_input_text)\n",
    "    lstm_out, hidden_state = self.lstm(embeds, hidden_state)\n",
    "    # Stack up LSTM outputs, apply dropout\n",
    "    lstm_out = lstm_out[-1,:,:]\n",
    "    lstm_out = self.dropout(lstm_out)\n",
    "    # Insert an additional fully connected when combining with other inputs\n",
    "    if self.dense_size == 0:\n",
    "        out = self.fc(lstm_out)\n",
    "    else:\n",
    "        dense_out = self.fc1(lstm_out)\n",
    "        out = self.fc2(dense_out)\n",
    "    # Softmax\n",
    "    logps = self.softmax(out)\n",
    "\n",
    "    return logps, hidden_state\n",
    "      \n",
    "     \n",
    "# Define LSTM Tokenizer\n",
    "def tokenizer_lstm(X, vocab, seq_len, padding):\n",
    "  '''\n",
    "  Returns tokenized tensor with left/right padding at the specified sequence length\n",
    "  '''\n",
    "  X_tmp = np.zeros((len(X), seq_len), dtype=np.int64)\n",
    "  for i, text in enumerate(X):\n",
    "    tokens = tokenize_text(text, 3) \n",
    "    token_ids = [vocab[word] for word in tokens]\n",
    "    end_idx = min(len(token_ids), seq_len)\n",
    "    if padding == 'right':\n",
    "      X_tmp[i,:end_idx] = token_ids[:end_idx]\n",
    "    elif padding == 'left':\n",
    "      start_idx = max(seq_len - len(token_ids), 0)\n",
    "      X_tmp[i,start_idx:] = token_ids[:end_idx]\n",
    "\n",
    "  return torch.tensor(X_tmp, dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Define a DataSet Class which simply return (x, y) pair\n",
    "class SimpleDataset(Dataset):\n",
    "  def __init__(self, x, y):\n",
    "    self.datalist=[(x[i], y[i]) for i in range(len(y))]\n",
    "  def __len__(self):\n",
    "    return len(self.datalist)\n",
    "  def __getitem__(self,idx):\n",
    "    return self.datalist[idx]\n",
    "      \n",
    "# Data Loader\n",
    "def create_data_loader(X, y, indices, batch_size, shuffle):\n",
    "  X_sampled = np.array(X, dtype=object)[indices]\n",
    "  y_sampled = np.array(y)[indices].astype(int)\n",
    "  dataset = SimpleDataset(X_sampled, y_sampled)\n",
    "  loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "  return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def train_cycles(X_all, y_all, vocab, num_samples, model_type, epochs, patience, batch_size, seq_len, lr, clip, log_level):\n",
    "  result = pd.DataFrame(columns=['Accuracy', 'F1(macro)', 'Total_Time', 'ms/text'], index=num_samples)\n",
    "\n",
    "  for n in num_samples:\n",
    "    print(\"\")\n",
    "    logging.info(\"############### Start training for %d samples ###############\" %n)\n",
    "\n",
    "    # Stratified sampling\n",
    "    train_size = n / len(y_all)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=train_size, test_size=train_size*0.2)\n",
    "    train_indices, valid_indices = next(sss.split(X_all, y_all))\n",
    "\n",
    "    # Sample input data\n",
    "    train_loader = create_data_loader(X_all, y_all, train_indices, batch_size, True)\n",
    "    valid_loader = create_data_loader(X_all, y_all, valid_indices, batch_size, False)\n",
    "\n",
    "    if model_type == 'LSTM':\n",
    "      model = LstmTextClassifier(len(vocab)+1, embed_size=512, lstm_size=1024, dense_size=0, output_size = 4, lstm_layers=4, dropout=0.2)\n",
    "      model.embedding.weight.data.uniform_(-1, 1)\n",
    "\n",
    "    start_time = time.perf_counter() # use time.process_time() for CPU time\n",
    "    acc, f1, model_trained = train_nn_model(model, model_type, train_loader, valid_loader, vocab, epochs, patience, batch_size, seq_len, lr, clip, log_level)\n",
    "    end_time = time.perf_counter() # use time.process_time() for CPU time\n",
    "    duration = end_time - start_time\n",
    "    logging.info(\"Process Time (sec): {}\".format(duration))\n",
    "    result.loc[n] = (round(acc,4), round(f1,4), duration, duration/n*1000)\n",
    "\n",
    "  return result, model_trained\n",
    "\n",
    "# Define metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def metric(y_true, y_pred):\n",
    "  acc = accuracy_score(y_true, y_pred)\n",
    "  f1 = f1_score(y_true, y_pred, average='macro')\n",
    "  return acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW as AdamW_HF, get_linear_schedule_with_warmup\n",
    "\n",
    "def train_nn_model(model, model_type, train_loader, valid_loader, vocab, epochs, patience, batch_size, seq_len, lr, clip, log_level):\n",
    "    # Set variables\n",
    "    num_total_opt_steps = int(len(train_loader) * epochs)\n",
    "    eval_every = len(train_loader) // 5\n",
    "    warm_up_proportion = 0.1\n",
    "    logging.info('Total Training Steps: {} ({} batches x {} epochs)'.format(num_total_opt_steps, len(train_loader), epochs))\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW_HF(model.parameters(), lr=lr, correct_bias=False) \n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_total_opt_steps*warm_up_proportion, num_training_steps=num_total_opt_steps)  # PyTorch scheduler\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # Set Train Mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialise\n",
    "    acc_train, f1_train, loss_train, acc_valid, f1_valid, loss_valid = [], [], [], [], [], []\n",
    "    best_f1, early_stop, steps = 0, 0, 0\n",
    "    class_names = ['xx-24','25-34', '35-49', '50+']\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "        logging.info('================     epoch {}     ==============='.format(epoch+1))\n",
    "\n",
    "        #################### Training ####################\n",
    "        # Initialise\n",
    "        loss_tmp, loss_cnt = 0, 0\n",
    "        y_pred_tmp, y_truth_tmp = [], []\n",
    "        hidden = model.init_hidden(batch_size) if model_type == \"LSTM\" else None\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            text_batch, labels = batch\n",
    "            # Skip the last batch of which size is not equal to batch_size\n",
    "            if labels.size(0) != batch_size:\n",
    "                break\n",
    "            steps += 1\n",
    "           \n",
    "            # Reset gradient\n",
    "            model.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Initialise after the previous training\n",
    "            if steps % eval_every == 1:\n",
    "                y_pred_tmp, y_truth_tmp = [], []\n",
    "\n",
    "            if model_type == \"LSTM\":\n",
    "                # Tokenize the input and move to device\n",
    "                text_batch = tokenizer_lstm(text_batch, vocab, seq_len, padding='left').transpose(1,0).to(device)\n",
    "                labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
    "\n",
    "                # Creating new variables for the hidden state to avoid backprop entire training history\n",
    "                hidden = tuple([each.data for each in hidden])\n",
    "                for each in hidden:\n",
    "                    each.to(device)\n",
    "\n",
    "                # Get output and hidden state from the model, calculate the loss\n",
    "                logits, hidden = model(text_batch, hidden)\n",
    "                loss = criterion(logits, labels)\n",
    "            \n",
    "            y_pred_tmp.extend(np.argmax(F.softmax(logits, dim=1).cpu().detach().numpy(), axis=1))\n",
    "            y_truth_tmp.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Back prop\n",
    "            loss.backward()\n",
    "\n",
    "            # Training Loss\n",
    "            loss_tmp += loss.item()\n",
    "            loss_cnt += 1\n",
    "\n",
    "            # Clip the gradient to prevent the exploading gradient problem in RNN/LSTM\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            # Update Weights and Learning Rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "            #################### Evaluation ####################\n",
    "            if (steps % eval_every == 0) or ((steps % eval_every != 0) and (steps == len(train_loader))):\n",
    "                # Evaluate Training\n",
    "                acc, f1 = metric(y_truth_tmp, y_pred_tmp)\n",
    "                acc_train.append(acc)\n",
    "                f1_train.append(f1)\n",
    "                loss_train.append(loss_tmp/loss_cnt)\n",
    "                loss_tmp, loss_cnt = 0, 0\n",
    "\n",
    "                # y_pred_tmp = np.zeros((len(y_valid), 5))\n",
    "                y_truth_tmp, y_pred_tmp = [], []\n",
    "\n",
    "                # Move to Evaluation Mode\n",
    "                model.eval()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    for i, batch in enumerate(valid_loader):\n",
    "                        text_batch, labels = batch\n",
    "                        # Skip the last batch of which size is not equal to batch_size\n",
    "                        if labels.size(0) != batch_size:\n",
    "                            break\n",
    "\n",
    "                        if model_type == \"LSTM\":\n",
    "                            # Tokenize the input and move to device\n",
    "                            text_batch = tokenizer_lstm(text_batch, vocab, seq_len, padding='left').transpose(1,0).to(device)\n",
    "                            labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
    "\n",
    "                            # Creating new variables for the hidden state to avoid backprop entire training history\n",
    "                            hidden = tuple([each.data for each in hidden])\n",
    "                            for each in hidden:\n",
    "                                each.to(device)\n",
    "\n",
    "                            # Get output and hidden state from the model, calculate the loss\n",
    "                            logits, hidden = model(text_batch, hidden)\n",
    "                            loss = criterion(logits, labels)\n",
    "                        loss_tmp += loss.item()\n",
    "                        loss_cnt += 1\n",
    "\n",
    "                        y_pred_tmp.extend(np.argmax(F.softmax(logits, dim=1).cpu().detach().numpy(), axis=1))\n",
    "                        y_truth_tmp.extend(labels.cpu().numpy())\n",
    "                        # logger.debug('validation batch: {}, val_loss: {}'.format(i, loss.item() / len(valid_loader)))\n",
    "\n",
    "                acc, f1 = metric(y_truth_tmp, y_pred_tmp)\n",
    "                logging.debug(\"Epoch: {}/{}, Step: {}, Loss: {:.4f}, Acc: {:.4f}, F1: {:.4f}\".format(epoch+1, epochs, steps, loss_tmp, acc, f1))\n",
    "                acc_valid.append(acc)\n",
    "                f1_valid.append(f1)\n",
    "                loss_valid.append(loss_tmp/loss_cnt)\n",
    "                loss_tmp, loss_cnt = 0, 0\n",
    "\n",
    "                # Back to train mode\n",
    "                model.train()\n",
    "\n",
    "        #################### End of each epoch ####################\n",
    "\n",
    "        # Show the last evaluation metrics\n",
    "        logging.info('Epoch: %d, Loss: %.4f, Acc: %.4f, F1: %.4f, LR: %.2e' % (epoch+1, loss_valid[-1], acc_valid[-1], f1_valid[-1], scheduler.get_last_lr()[0]))\n",
    "\n",
    "        # Plot Confusion Matrix\n",
    "        y_truth_class = [class_names[int(idx)] for idx in y_truth_tmp]\n",
    "        y_predicted_class = [class_names[int(idx)] for idx in y_pred_tmp]\n",
    "        \n",
    "        titles_options = [(\"Actual Count\", None), (\"Normalised\", 'true')]\n",
    "        for title, normalize in titles_options:\n",
    "            disp = skplt.metrics.plot_confusion_matrix(y_truth_class, y_predicted_class, normalize=normalize, title=title, x_tick_rotation=75)\n",
    "        plt.show()\n",
    "\n",
    "        # plot training performance\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6))\n",
    "        ax1.set_title(\"Losses\")\n",
    "        ax1.set_xlabel(\"Validation Cycle\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.plot(loss_train, 'b-o', label='Train Loss')\n",
    "        ax1.plot(loss_valid, 'r-o', label='Valid Loss')\n",
    "        ax1.legend(loc=\"upper right\")\n",
    "        \n",
    "        ax2.set_title(\"Evaluation\")\n",
    "        ax2.set_xlabel(\"Validation Cycle\")\n",
    "        ax2.set_ylabel(\"Score\")\n",
    "        ax2.set_ylim(0,1)\n",
    "        ax2.plot(acc_train, 'y-o', label='Accuracy (train)')\n",
    "        ax2.plot(f1_train, 'y--', label='F1 Score (train)')\n",
    "        ax2.plot(acc_valid, 'g-o', label='Accuracy (valid)')\n",
    "        ax2.plot(f1_valid, 'g--', label='F1 Score (valid)')\n",
    "        ax2.legend(loc=\"upper left\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # If improving, save the number. If not, count up for early stopping\n",
    "        if best_f1 < f1_valid[-1]:\n",
    "            early_stop = 0\n",
    "            best_f1 = f1_valid[-1]\n",
    "        else:\n",
    "            early_stop += 1\n",
    "\n",
    "        # Early stop if it reaches patience number\n",
    "        if early_stop >= patience:\n",
    "            break\n",
    "\n",
    "        # Prepare for the next epoch\n",
    "        if device == 'cuda:0':\n",
    "            torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "\n",
    "    return acc, f1, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    paranoia is settin'in sumbuddy's gonna get hur...\n",
       "1    damn these games kicked my ass today huge wate...\n",
       "2    it is better to conquer yourself than to win t...\n",
       "3    its hot over here lol aloha everyone heading o...\n",
       "4    he's the lord of all the earth the maker of al...\n",
       "Name: Content, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Content'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "# Create vocab\n",
    "def create_vocab(messages, show_graph=False):\n",
    "    corpus = []\n",
    "    for message in messages:\n",
    "        tokens = tokenize_text(message, 3) # Use option 3\n",
    "        corpus.extend(tokens)\n",
    "    logging.info(\"The number of all words: {}\".format(len(corpus)))\n",
    "\n",
    "    # Create Counter\n",
    "    counts = Counter(corpus)\n",
    "    logging.info(\"The number of unique words: {}\".format(len(counts)))\n",
    "\n",
    "    # Create BoW\n",
    "    bow = sorted(counts, key=counts.get, reverse=True)\n",
    "    logging.info(\"Top 40 frequent words: {}\".format(bow[:40]))\n",
    "\n",
    "    # Indexing vocabrary, starting from 1.\n",
    "    vocab = {word: ii for ii, word in enumerate(counts, 1)}\n",
    "    id2vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    return vocab\n",
    "\n",
    "vocab= create_vocab(preprocessed, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/itadmin/.local/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]/tmp/ipykernel_201366/2977133332.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
      "/tmp/ipykernel_201366/2977133332.py:106: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.int64).to(device)\n",
      "/tmp/ipykernel_201366/2977133332.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels, dtype=torch.int64).to(device)\n"
     ]
    }
   ],
   "source": [
    "num_samples = [1000, 5000, 9500]\n",
    "epochs=5\n",
    "patience=3\n",
    "batch_size=64\n",
    "seq_len = 30\n",
    "lr=3e-4\n",
    "clip=5\n",
    "log_level=logging.DEBUG\n",
    "\n",
    "# Run!\n",
    "result_lstm, model_trained_lstm = train_cycles(df['Content'], df['age'], vocab, num_samples, 'LSTM', epochs, patience, batch_size, seq_len, lr, clip, log_level)\n",
    "\n",
    "# Save the model and show the result\n",
    "torch.save(model_trained_lstm.state_dict(),'stocktwit_lstm.dict')\n",
    "result_lstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
