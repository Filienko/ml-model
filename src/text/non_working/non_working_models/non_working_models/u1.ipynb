{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 23:15:40.836974: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 93763584 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import bert\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"age.csv\")\n",
    "\n",
    "# Define the labels and corresponding texts\n",
    "labels = df['age'].values\n",
    "texts = df['Content'].values\n",
    "\n",
    "# Tokenize the texts\n",
    "tokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenizer(vocabulary_file, to_lower_case)\n",
    "\n",
    "def encode_text(text, tokenizer, max_len=512):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = tokens[:max_len-2]\n",
    "    input_sequence = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    padding = [0] * (max_len - len(input_ids))\n",
    "    input_ids += padding\n",
    "    input_mask += padding\n",
    "    segment_ids = [0] * max_len\n",
    "    return input_ids, input_mask, segment_ids\n",
    "\n",
    "encoded_texts = [encode_text(text, tokenizer) for text in texts]\n",
    "encoded_texts = np.array(encoded_texts)\n",
    "input_ids = encoded_texts[:, 0]\n",
    "input_masks = encoded_texts[:, 1]\n",
    "segment_ids = encoded_texts[:, 2]\n",
    "\n",
    "# Convert the labels to one-hot encoding\n",
    "labels = pd.get_dummies(labels).values\n",
    "\n",
    "# Split the data into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(encoded_texts, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# Build the model\n",
    "input_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"input_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(512,), dtype=tf.int32, name=\"segment_ids\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 23:19:01.779691: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 15627264 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bert_model = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/2')\n",
    "\n",
    "pooled_output, sequence_output = bert_layer([input_ids, input_mask, segment_ids])\n",
    "\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(pooled_output)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "output = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=[input_ids, input_mask, segment_ids], outputs=output)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FullTokenizer' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode(texts, add_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m input_ids_test \u001b[39m=\u001b[39m texts\u001b[39m.\u001b[39mtensor([input_ids])\n\u001b[1;32m      4\u001b[0m \u001b[39m# Generate the input mask\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FullTokenizer' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(texts, add_special_tokens=True)\n",
    "input_ids_test = texts.tensor([input_ids])\n",
    "\n",
    "# Generate the input mask\n",
    "input_mask_test = texts.ones_like(input_ids_test)\n",
    "\n",
    "# Generate the segment IDs\n",
    "segment_ids_test = texts.zeros_like(input_ids_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_ids_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit([input_ids_train, input_mask_train, segment_ids_train], y_train, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, validation_data\u001b[39m=\u001b[39m([input_ids_test, input_mask_test, segment_ids_test], y_test))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_ids_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit([input_ids_train, input_mask_train, segment_ids_train], y_train, epochs=10, batch_size=32, validation_data=([input_ids_test, input_mask_test, segment_ids_test], y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "scores = model.evaluate([input_ids_test, input_mask_test, segment_ids_test], y_test)\n",
    "print(\"Accuracy:\", scores[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
